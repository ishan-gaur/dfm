import torch
from torch import nn
from torch.nn import functional as F
from dfm.generative_modeling import TransitionModel
from dfm.predictive_modeling import PredictiveModel
from transformers import PreTrainedTokenizerBase
from contextlib import contextmanager
from typing import Optional, List
import warnings


class TokenizerTranslator(nn.Module):
    def __init__(
        self,
        tokenizer_src: PreTrainedTokenizerBase,
        tokenizer_tgt: PreTrainedTokenizerBase,
        target_output_dim: Optional[int] = None,
    ):
        super().__init__()
        if target_output_dim is None:
            target_output_dim = tokenizer_tgt.vocab_size
        convert_SrcTgt = torch.zeros(tokenizer_src.vocab_size, target_output_dim)
        src_vocab = set(tokenizer_src.vocab.keys())
        tgt_vocab = set(tokenizer_tgt.vocab.keys())
        shared_vocab = src_vocab & tgt_vocab
        if not shared_vocab:
            raise ValueError(
                f"Source and target tokenizer vocabs have no tokens in common"
            )
        self.vocab_eq = tgt_vocab.issubset(src_vocab)

        for tok in shared_vocab:
            i = tokenizer_src.vocab[tok]
            j = tokenizer_tgt.vocab[tok]
            convert_SrcTgt[i, j] = 1.0
        self.register_buffer("convert_SrcTgt", convert_SrcTgt)

    def forward(self, x_SPSrc: torch.Tensor) -> torch.Tensor:
        x_SPTgt = torch.matmul(x_SPSrc, self.convert_SrcTgt[None, ...])
        return x_SPTgt.to(x_SPSrc.dtype)

    def reverse(self, x_SPTgt: torch.Tensor) -> torch.Tensor:
        if not self.vocab_eq:
            warnings.warn(
                "Tokens are being converted to a target vocab that does not include the src as a subset",
                UserWarning,
            )
        x_SPSrc = torch.matmul(x_SPTgt, self.convert_SrcTgt.T[None, ...])
        return x_SPSrc.to(x_SPTgt.dtype)


class TAG(TransitionModel):
    # TAG and DEG are basically ways to efficiently compute the vector p(y|x_const)
    # Therefore, we don't make the predictive models deal with that kind of query
    def __init__(
        self,
        gen_model: TransitionModel,
        pred_model: PredictiveModel,
    ):
        super().__init__(
            model=gen_model.model,
            tokenizer=gen_model.tokenizer,
            logit_formatter=gen_model.logit_formatter,
        )
        # main stipulation here is that the predictive model has to take OHE as input
        self.gen_model = gen_model
        self.pred_model = pred_model
        self.gen_to_pred_space = TokenizerTranslator(
            self.tokenizer,
            self.pred_model.tokenizer,
            self.pred_model.input_dim,  # TODO[pi] need to add this to the ABC
        )

        # Detect whether gen tokenizer adds CLS/EOS that pred tokenizer doesn't have
        gen_has_cls = getattr(self.tokenizer, "cls_token_id", None) is not None
        gen_has_eos = getattr(self.tokenizer, "eos_token_id", None) is not None
        pred_has_cls = getattr(self.pred_model.tokenizer, "cls_token_id", None) is not None
        pred_has_eos = getattr(self.pred_model.tokenizer, "eos_token_id", None) is not None
        self._strip_prefix = 1 if (gen_has_cls and not pred_has_cls) else 0
        self._strip_suffix = 1 if (gen_has_eos and not pred_has_eos) else 0

    def forward(
        self, seq_SP: torch.LongTensor
    ):  # seq_SP is assumed to be natively tokenized for the generative model
        logp_xtilde_g_x_SPT = self.gen_model.get_log_probs(seq_SP)
        with torch.enable_grad():
            ohe_seq_SPT = (
                F.one_hot(seq_SP, num_classes=self.gen_model.logit_formatter.output_dim)
                .float()
                .requires_grad_(True)
            )
            # Strip CLS/EOS positions if gen and pred tokenizers disagree
            if self._strip_suffix > 0:
                ohe_inner = ohe_seq_SPT[:, self._strip_prefix : -self._strip_suffix, :]
            elif self._strip_prefix > 0:
                ohe_inner = ohe_seq_SPT[:, self._strip_prefix :, :]
            else:
                ohe_inner = ohe_seq_SPT

            # The conversion is done separately so that the gradients accumulate in the generative model's token space
            logp_y_g_x_S = self.pred_model.target_log_probs_given_ohe(
                self.gen_to_pred_space(ohe_inner)
            )  # TODO[pi] the user should set the class/threshold so this doesn't logpy_g_x_SC
            logp_y_g_x_S.sum().backward()
            logp_y_g_xtilde_SPT = ohe_seq_SPT.grad
        return logp_y_g_xtilde_SPT + logp_xtilde_g_x_SPT

    # Since this is a TransitionModel, the transition_log_probs function is automatically generated by the parent class


class DEG(TransitionModel):
    # TAG and DEG are basically ways to efficiently compute the vector p(y|x_const)
    # Therefore, we don't make the predictive models deal with that kind of query
    def __init__(
        self,
        gen_model: TransitionModel,
        pred_model: PredictiveModel,
    ):
        super().__init__(
            model=gen_model.model,
            tokenizer=gen_model.tokenizer,
            logit_formatter=gen_model.logit_formatter,
        )
        # main stipulation here is that the predictive model has to take OHE as input
        self.gen_model = gen_model
        self.pred_model = pred_model
        self.positions_to_score_S = None

    # TODO[pi] with all these different things we have to mix in, I'm wondering if
    # the context manager approach was really the right one
    @contextmanager
    def at_position(self, positions_to_score_S: List[int]):
        """
        positions_to_score_S is a list that gives the index at each sequence to try to sample
        If a sequence does not need to be sampled, pass None for that index in the list
        """
        self.positions_to_score_S = positions_to_score_S
        try:
            yield self
        finally:
            self.positions_to_score_S = None

    # TODO[pi] need forward that is fully batched for predictor, sequence-wise batched, and does things one at a time
    def forward(self, seq_SP: torch.LongTensor):
        if self.positions_to_score_S is None:
            raise ValueError(
                "Need to call ``self.at_position(positions_to_score_S)`` to provide the position to score for each sequence"
            )
        logp_xtilde_g_x_SPT = self.gen_model.get_log_probs(seq_SP)
        logp_y_g_xtilde_SPT = torch.zeros_like(logp_xtilde_g_x_SPT)
        n_tok = self.tokenizer.vocab_size
        for s, p in enumerate(self.positions_to_score_S):
            if p is None:
                continue
            # for simplicity just try all possible tokens--including special ones TODO[pi] we could instead use the
            # logitformatter mask in order to only try the valid transitions here
            seq_XP = (
                seq_SP[s].unsqueeze(0).repeat(n_tok, 1)
            )  # X is the index over tokens we're trying
            seq_XP[:, p] = torch.arange(n_tok)
            logp_y_g_xtilde_X = self.pred_model.get_log_prob_target_from_seq(seq_XP)
            logp_y_g_xtilde_SPT[s, p, :n_tok] = logp_y_g_xtilde_X
            # Don't need to take care of making the others -inf since the logit_formatter will take care of the invalid ones (including the invalid ones we tested lol)
        return logp_y_g_xtilde_SPT + logp_xtilde_g_x_SPT
