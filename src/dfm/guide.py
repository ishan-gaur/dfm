import torch
from torch import nn
from torch.nn import functional as F
from dfm.generative_modeling import TransitionModel
from dfm.predictive_model import PredictiveModel
from transformers import PreTrainedTokenizerBase
from contextlib import contextmanager
from typing import Optional, List
import warnings


class TokenizerTranslator(nn.Module):
    def __init__(
        self,
        tokenizer_src: PreTrainedTokenizerBase,
        tokenizer_tgt: PreTrainedTokenizerBase,
        target_output_dim: Optional[int] = None,
    ):
        super().__init__()
        if target_output_dim is None:
            target_output_dim = tokenizer_tgt.vocab_size
        convert_SrcTgt = torch.zeros(tokenizer_src.vocab_size, target_output_dim)
        src_vocab = set(tokenizer_src.vocab.keys())
        tgt_vocab = set(tokenizer_tgt.vocab.keys())
        if not src_vocab.issubset(tgt_vocab):
            raise ValueError(
                f"Source tokenizer vocab {src_vocab} must be a subset of the target tokenizer vocab {tgt_vocab}"
            )
        self.vocab_eq = tgt_vocab.issubset(src_vocab)

        for tok in src_vocab:
            i = tokenizer_src.vocab[tok]
            j = tokenizer_tgt.vocab[tok]
            convert_SrcTgt[i, j] = 1.0
        self.register_buffer("convert_SrcTgt", convert_SrcTgt)

    def forward(self, x_SPSrc: torch.Tensor) -> torch.Tensor:
        x_SPTgt = torch.matmul(x_SPSrc, self.convert_SrcTgt[None, ...])
        return x_SPTgt.to(x_SPSrc.dtype)

    def reverse(self, x_SPTgt: torch.Tensor) -> torch.Tensor:
        if not self.vocab_eq:
            warnings.warn(
                "Tokens are being converted to a target vocab that does not include the src as a subset",
                UserWarning,
            )
        x_SPSrc = torch.matmul(x_SPTgt, self.convert_SrcTgt.T[None, ...])
        return x_SPSrc.to(x_SPTgt.dtype)


class TAG(TransitionModel):
    # TAG and DEG are basically ways to efficiently compute the vector p(y|x_const)
    # Therefore, we don't make the predictive models deal with that kind of query
    def __init__(
        self,
        gen_model: TransitionModel,
        pred_model: PredictiveModel,
    ):
        super().__init__()
        # main stiputation here is that the predictive model has to take OHE as input
        self.gen_model = gen_model
        self.logit_formatter = self.gen_model.logit_formatter
        self.tokenizer = self.gen_model.tokenizer

        self.pred_model = pred_model
        self.gen_to_pred_space = TokenizerTranslator(
            self.tokenizer,
            self.pred_model.tokenizer,
            self.pred_model.input_dim,  # TODO[pi] need to add this to the ABC
        )

    def forward(
        self, seq_SP: torch.LongTensor
    ):  # seq_SP is assumed to be natively tokenized for the generative model
        logp_xtilde_g_x_SPT = self.gen_model.transition_log_probs(seq_SP)
        # ah, just make the predictive model specify its own tokenizer too--normally just the same as the predictive model
        # generally for these predictors, you'll have a OHE at somepoint and you want to make that as a leaf and return it
        with torch.enable_grad():
            ohe_seq_SPT = F.one_hot(
                seq_SP, num_classes=self.gen_model.logit_formatter.output_dim
            ).requires_grad_(True)
            # TODO[pi] we can automatically generate the str_to_ohe function based on the tokenizer in the PredictiveModel
            # ohe_seq_SPT = self.pred_model.str_to_ohe(str_seq_SP).requires_grad_(True)
            ohe_seq_SPT.grad.zero_()
            # The conversion is done separately so that the gradients accumulate in the generative model's token space
            logp_y_g_x_S = self.pred_model(
                self.gen_to_pred_space(ohe_seq_SPT)
            )  # TODO[pi] the user should set the class/threshold so this doesn't logpy_g_x_SC
            logp_y_g_x_S.sum().backward()
            logp_y_g_xtilde_SPT = ohe_seq_SPT.grad
        return logp_y_g_xtilde_SPT + logp_xtilde_g_x_SPT

    # Since this is a TransitionModel, the transition_log_probs function is automatically generated by the parent class


class DEG(TransitionModel):
    # TAG and DEG are basically ways to efficiently compute the vector p(y|x_const)
    # Therefore, we don't make the predictive models deal with that kind of query
    def __init__(
        self,
        gen_model: TransitionModel,
        pred_model: PredictiveModel,
    ):
        super().__init__()
        # main stipulation here is that the predictive model has to take OHE as input
        self.gen_model = gen_model
        self.logit_formatter = self.gen_model.logit_formatter
        self.tokenizer = self.gen_model.tokenizer

        self.pred_model = pred_model
        self.gen_to_pred_space = TokenizerTranslator(
            self.tokenizer,
            self.pred_model.tokenizer,
            self.pred_model.input_dim,  # TODO[pi] need to add this to the ABC
        )
        self.positions_to_score_S = None

    # TODO[pi] with all these different things we have to mix in, I'm wondering if
    # the context manager approach was really the right one
    @contextmanager
    def at_position(self, positions_to_score_S: List[int]):
        """
        positions_to_score_S is a list that gives the index at each sequence to try to sample
        If a sequence does not need to be sampled, pass None for that index in the list
        """
        self.positions_to_score_S = positions_to_score_S
        try:
            yield self
        finally:
            self.positions_to_score_S = None

    # TODO[pi] need forward that is fully batched for predictor, sequence-wise batched, and does things one at a time
    def forward(self, seq_SP: torch.LongTensor):
        if self.positions_to_score_S is None:
            raise ValueError(
                "Need to call ``self.at_position(positions_to_score_S)`` to provide the position to score for each sequence"
            )
        logp_xtilde_g_x_SPT = self.gen_model.transition_log_probs(seq_SP)
        logp_y_g_xtilde_SPT = torch.zeros_like(logp_xtilde_g_x_SPT)
        n_tok = self.tokenizer.vocab_size
        for s, p in enumerate(self.positions_to_score_S):
            if p is None:
                continue
            # for simplicity just try all possible tokens--including special ones TODO[pi] we could instead use the
            # logitformatter mask in order to only try the valid transitions here
            seq_XP = (
                seq_SP[s].unsqueeze(0).repeat(n_tok, 1)
            )  # X is the number of tokens we're trying
            seq_XP[:, p] = torch.arange(n_tok)
            ohe_seq_XPT = self.gen_to_pred_space(
                F.one_hot(seq_XP, num_classes=self.gen_model.logit_formatter.output_dim)
            )
            logp_y_g_xtilde_X = self.pred_model(ohe_seq_XPT)
            logp_y_g_xtilde_SPT[s, p, :n_tok] = logp_y_g_xtilde_X
            # Don't need to take care of making the others -inf since the logit_formatter will take care of the invalid ones (including the invalid ones we tested lol)
        return logp_y_g_xtilde_SPT + logp_xtilde_g_x_SPT
